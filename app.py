import os
import textwrap
from typing import Any, Dict, List

import streamlit as st
from openai import OpenAI
from google import genai
from google.genai import types
from pinecone import Pinecone


# =========================
# Clients / Indices
# =========================
def get_clients_no_firebase():
    pinecone_api_key = os.getenv("PINECONE_API_KEY")
    openrouter_api_key = os.getenv("OPENROUTER_API_KEY")
    gemini_api_key = os.getenv("GEMINI_API_KEY")

    openai_client = OpenAI()

    gemini_client = genai.Client(api_key=gemini_api_key)

    openrouter_client = OpenAI(
        base_url="https://openrouter.ai/api/v1",
        api_key=openrouter_api_key,
    )

    pc = Pinecone(api_key=pinecone_api_key)
    dense_index = pc.Index("antiqua-dense")
    sparse_index = pc.Index("antiqua-sparse")
    return openai_client, gemini_client, openrouter_client, pc, dense_index, sparse_index


OPENAI, GEMINI, OPENROUTER, PC, DENSE_IDX, SPARSE_IDX = get_clients_no_firebase()

# =========================
# Helpers (translation, embed, search)
# =========================
def translate_query(query_ja: str) -> str:

    full_user_content = f"CURRENT QUERY TO REFORMULATE:\n{query_ja}".strip()

    system_prompt = textwrap.dedent("""
        You are a highly specialized query reformulator for a vector database focused on Western Classical literature (Ancient Greek & Latin). Your sole purpose is to convert a user's intent into a perfect, self-contained English search query.

        You will receive the user's intent in a structured format containing:
        1.  `CURRENT QUERY TO REFORMULATE`: The user's latest query, which is your main focus.

        **Your Four-Step Process:**
        1.  **Analyze Intent:** Fully grasp the user's core question from the `CURRENT QUERY` and surrounding context.
        2.  **Translate to English:** Accurately translate the Japanese query into clear English. This is the most critical first step.
        3.  **Format for Search:** Formulate the final query as a concise, unambiguous string, applying domain-specific knowledge (e.g., standard names like "Thucydides", "Nicomachean Ethics"; technical terms like "arete (virtue, excellence)").

        **CRITICAL OUTPUT RULE:**
        - Your response MUST be the final English query string and nothing else.
        - DO NOT include any prefixes, explanations, or conversational text. Just the query.
    """)

    final_user_prompt = f"""Please reformulate a search query based on the following information. Your output must be only the final English query string.\n---\n{full_user_content}"""

    res = OPENROUTER.chat.completions.create(
        model="google/gemini-2.0-flash-001",
        messages=[
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": final_user_prompt},
        ],
    )
    return res.choices[0].message.content.strip()

def embed_query(query_en: str) -> List[float]:
    resp = GEMINI.models.embed_content(
        contents=query_en,
        model="gemini-embedding-001",
        config=types.EmbedContentConfig(task_type="RETRIEVAL_QUERY"),
    )
    return resp.embeddings[0].values

def search_pinecone(vec: List[float], query_en: str, k: int = 20, filters=None):
    if filters is None:
        filters = {}
    dense = DENSE_IDX.query(
        namespace="__default__",
        vector=vec,
        top_k=k,
        filter=filters,
        include_metadata=True,
        include_values=False,
    )
    sparse = SPARSE_IDX.search(
        namespace="__default__",
        query={"top_k": k, "inputs": {"text": query_en}, "filter": filters},
        fields=["id", "author", "work", "chunk_text", "text"],
    )
    return dense, sparse


def merge_hits(resp1, resp2):
    def _norm(resp):
        if "result" in resp and "hits" in resp["result"]:
            for h in resp["result"]["hits"]:
                yield {
                    "id": h["_id"],
                    "score": h["_score"],
                    "author": h["fields"].get("author"),
                    "work": h["fields"].get("work"),
                    "chunk_text": h["fields"].get("chunk_text"),
                    "text": h["fields"].get("text"),
                }
        else:
            for h in resp.get("matches", []):
                m = h["metadata"]
                yield {
                    "id": h["id"],
                    "score": h["score"],
                    "author": m.get("author"),
                    "work": m.get("work"),
                    "chunk_text": m.get("chunk_text"),
                    "text": m.get("text"),
                }

    combined = {h["id"]: h for h in _norm(resp1)}
    combined.update({h["id"]: h for h in _norm(resp2)})
    return sorted(combined.values(), key=lambda x: x["score"], reverse=True)

def _author_str(v):
    if isinstance(v, list):
        return ", ".join(v)
    return v or ""

def make_rank_payload(doc: Dict[str, Any]) -> str:
    # è‹±èªã®ã¿ãƒ»çŸ­ãï¼šè‘—è€…ï¼è‘—ä½œã‚’å…ˆé ­ã«ã€æ¬¡ã«è¦ç´„
    author = _author_str(doc.get("author"))
    work = doc.get("work", "")
    summary_en = doc.get("text", "")  # â† index.py ã§ã¯è‹±èªè¦ç´„ãŒ "text"
    # ãƒˆãƒ¼ã‚¯ãƒ³ç¯€ç´„ã®ãŸã‚æ”¹è¡Œã¯å°‘ãªã‚ãƒ»è¨˜å·åŒºåˆ‡ã‚Šã«
    return f"Author: {author} | Work: {work}\nSummary: {summary_en}"

def build_sources_html_from_rerank(rerank_data: List[Dict[str, Any]], doc_lookup) -> str:
    html = []
    for row in rerank_data:
        rid = str(row["document"].get("id", ""))
        doc = doc_lookup.get(rid, {}) 
        html.append(
            f"<details>"
            f"<summary>{(doc.get('id') or rid).replace('_', ' ')} / Score: {float(row['score'])}</summary>"
            f"<p>{doc.get('chunk_text','')}</p>"
            f"<p><strong>ã€Summary in Englishã€‘</strong><br/>{doc.get('text','')}</p>"
            f"</details>"
        )
    return "\n".join(html)


def build_sources_html_from_docs(docs: List[Dict[str, Any]]) -> str:
    html = []
    for d in docs:
        html.append(
            f"<details>"
            f"<summary>{d['id'].replace('_', ' ')} / Score: {float(d['score'])}</summary>"
            f"<p>{d.get('chunk_text','')}</p>"
            f"<p><strong>ã€Summary in Englishã€‘</strong><br/>{d.get('text','')}</p>"
            f"</details>"
        )
    return "\n".join(html)

# =========================
# Streamlit UI (main only; no sidebar)
# =========================
st.set_page_config(page_title="Antiqua æ¤œç´¢è©•ä¾¡", layout="wide")
st.title("ğŸ” Humanitext Antiqua æ¤œç´¢è©•ä¾¡")
st.caption("ã‚¯ã‚¨ãƒªã«åŸºã¥ãå€™è£œæ–‡è„ˆã‚’2ç¨®é¡è¡¨ç¤ºã—ã¾ã™ï¼ˆå†…å®¹ã®ã¿ï¼‰ã€‚")

# --- è©•ä¾¡ã‚¬ã‚¤ãƒ‰ï¼ˆexpanderè¡¨ç¤ºï¼‰ ---------------------------------------------
def render_evaluation_guide():
    guide_md = textwrap.dedent("""
    # è©•ä¾¡ã‚¬ã‚¤ãƒ‰

    **ç›®çš„**  
    åŒã˜ã‚¯ã‚¨ãƒªã«å¯¾ã™ã‚‹ã€ŒçµæœAï¼ˆå·¦ï¼‰ã€ã¨ã€ŒçµæœBï¼ˆå³ï¼‰ã€ã®ä¸Šä½å€™è£œï¼ˆTop5ï¼‰ã‚’æ¯”ã¹ã€ã©ã¡ã‚‰ãŒã‚ˆã‚Šé©åˆ‡ã«ç­”ãˆã¦ã„ã‚‹ã‹ã‚’ç°¡æ½”ã«åˆ¤æ–­ãã ã•ã„ã€‚

    ---  
    ## ä½•ã‚’è¦‹ã‚‹ã‹
    - **Top1 åˆè‡´åº¦**ï¼š1ä½ã®æ–‡è„ˆãŒã‚¯ã‚¨ãƒªã«ç›´æ’ƒã—ã¦ã„ã‚‹ã‹ã€‚  
    - **Top3 åˆè‡´åº¦**ï¼šä¸Šä½3ä»¶ã®ã¾ã¨ã¾ã‚Šï¼ˆçš„å¤–ã‚Œã®å°‘ãªã•ã€é‡è¤‡ã®å°‘ãªã•ã€é©åˆ‡ãªåºƒãŒã‚Šï¼‰ã€‚  
    - **Top5 åˆè‡´åº¦**ï¼šä¸Šä½5ä»¶ã®ç·åˆåŠ›ï¼ˆé–¢é€£æ€§ã®ä¸€è²«æ€§ã€ä¸è¦ãƒã‚¤ã‚ºã®å°‘ãªã•ã€é †ç•ªã®å¦¥å½“æ€§ï¼‰ã€‚  

    å„é …ç›®ã”ã¨ã« **A / B / å¼•åˆ† / ã©ã¡ã‚‰ã‚‚ä¸é©** ã®4æŠã§åˆ¤æ–­ã—ã¾ã™ã€‚

    ---  
    ## ã‚¯ã‚¨ãƒªã‚¿ã‚¤ãƒ—
    - **ã‚¿ã‚¤ãƒ—Aï¼ˆè‘—è€…ãƒ»è‘—ä½œã‚’æ˜è¨˜ï¼è¤‡æ•°å¯ï¼‰**  
      ä¾‹ï¼š*ãƒ—ãƒ©ãƒˆãƒ³ã€ãƒ‘ã‚¤ãƒ‰ãƒ³ã€ã®é­‚ã®ä¸æ­»*  
      â†’ ç†æƒ³ã¯ã€æ˜è¨˜ã—ãŸè‘—è€…ãƒ»è‘—ä½œï¼ˆã‚¢ãƒ³ã‚«ãƒ¼ï¼‰ãŒ **Top1 ã«å‡ºã‚‹**ã“ã¨ã€‚  
      â†’ ãŸã ã—ä»–è‘—è€…ãƒ»ä»–ä½œå“ã§ã‚‚ã€ã‚¯ã‚¨ãƒªå›ç­”ã« **æœ‰ç”¨ãªã‚‰è©•ä¾¡æ ¹æ‹ ã«å«ã‚ã¦ã‚ˆã„**ï¼ˆâ€œå…¥ã£ã¦ã„ã‚‹ã ã‘â€ã§æ©Ÿæ¢°çš„ã«Ã—ã«ã—ãªã„ï¼‰ã€‚

    - **ã‚¿ã‚¤ãƒ—Bï¼ˆè‘—è€…ã®ã¿æ˜è¨˜ï¼è¤‡æ•°å¯ï¼‰**  
      ä¾‹ï¼š*ã‚­ã‚±ãƒ­ã® officium ã®å®šç¾©*  
      â†’ æ˜è¨˜ã—ãŸè‘—è€…ç”±æ¥ãŒä¸Šä½ã«æ¥ã‚‹ã®ãŒæœ›ã¾ã—ã„ãŒã€æœ‰ç”¨ãªã‚‰é–¢é€£ä»–è‘—è€…ã‚‚å¯ã€‚

    - **ã‚¿ã‚¤ãƒ—Cï¼ˆè‘—è€…ãƒ»è‘—ä½œã‚’æ˜è¨˜ã—ãªã„ï¼‰**  
      ä¾‹ï¼š*ã‚¢ãƒ¬ãƒ†ãƒ¼ã¨ã‚¨ã‚¦ãƒ€ã‚¤ãƒ¢ãƒ‹ã‚¢ã®é–¢ä¿‚*  
      â†’ ãƒ†ãƒ¼ãƒã¸ã®ç›´æ’ƒåº¦ã¨é †ä½ã®é©åˆ‡ã•ã‚’é‡è¦–ï¼ˆç‰¹å®šä½œè€…ã®å‰æãªã—ï¼‰ã€‚

    ---  
    ## åˆ¤å®šã®ãƒ’ãƒ³ãƒˆ
    - **ç›´æ’ƒåº¦**ï¼šè³ªå•ã«ãã®ã¾ã¾ç­”ãˆã‚‹å†…å®¹ã‹ã€‚  
    - **ä¸è¦ãƒã‚¤ã‚º**ï¼šé–¢ä¿‚ã®è–„ã„å€™è£œãŒæ··ã˜ã£ã¦ã„ãªã„ã‹ã€‚  
    - **é †åº**ï¼šã‚ˆã‚Šé©åˆ‡ãªã‚‚ã®ãŒä¸Šä½ã«æ¥ã¦ã„ã‚‹ã€‚
    - **åºƒãŒã‚Š**ï¼šTop3/Top5ã§é‡è¤‡ã°ã‹ã‚Šã«ãªã‚‰ãšã€è£œåŠ©çš„è¦³ç‚¹ãŒé©åº¦ã«å«ã¾ã‚Œã¦ã„ã‚‹ã‹ã€‚

    ---  
    ## æ“ä½œæ‰‹é †
    1. ã‚¯ã‚¨ãƒªå…¥åŠ› â†’ **ã€Œæ¤œç´¢ã‚’å®Ÿè¡Œã€**ã‚’æŠ¼ã™ï¼ˆEnterã§ã¯é€ä¿¡ã•ã‚Œã¾ã›ã‚“ï¼‰ã€‚  
    2. å·¦ãŒ**çµæœA**ã€å³ãŒ**çµæœB**ã€‚  
    3. 3é …ç›®ï¼ˆTop1 / Top3 / Top5ï¼‰ãã‚Œãã‚Œã§ **A / B / å¼•åˆ† / ã©ã¡ã‚‰ã‚‚ä¸é©** ã‚’é¸ã¶ã€‚  
    4. æ¬¡ã®ã‚¯ã‚¨ãƒªã¸ã€‚  
    â€» A/BãŒã©ã®æ–¹å¼ã‹ã¯æ°—ã«ã—ãªã„ã§ãã ã•ã„ï¼ˆãƒ–ãƒ©ã‚¤ãƒ³ãƒ‰ï¼‰ã€‚

    ---  
    ## è¨˜éŒ²ï¼ˆCSVï¼‰
    å„è¡Œï¼š1ã‚¯ã‚¨ãƒª  
    - `query_type`ï¼ˆA / B / Cï¼‰  
    - `query_text`  
    - `top1_winner` / `top3_winner` / `top5_winner`ï¼ˆA / B / å¼•åˆ† / ã©ã¡ã‚‰ã‚‚ä¸é©ï¼‰  
    æœ€ä¸‹éƒ¨ã«**è‡ªç”±è¨˜è¿°ã‚³ãƒ¡ãƒ³ãƒˆ**ã‚‚ãŠé¡˜ã„ã—ã¾ã™ï¼ˆã¾ã¨ã‚ã¦1ã¤ï¼‰ã€‚
    """).strip()

    with st.expander("ğŸ“˜ è©•ä¾¡ã‚¬ã‚¤ãƒ‰ï¼ˆã‚¯ãƒªãƒƒã‚¯ã§é–‹ãï¼‰", expanded=False):
        st.markdown(guide_md)

# ä½¿ã„ã©ã“ã‚ï¼ˆã‚¿ã‚¤ãƒˆãƒ«ã®ç›´å¾Œãªã©ï¼‰
render_evaluation_guide()

st.subheader("æ¤œç´¢æ¡ä»¶")
query = st.text_area("ã‚¯ã‚¨ãƒªï¼ˆæ—¥æœ¬èªã§OKï¼‰", placeholder="ä¾‹ï¼‰ã‚­ã‚±ãƒ­ã® officium ã®å®šç¾©ã¯ï¼Ÿ", height=100)
run_btn = st.button("ğŸ” æ¤œç´¢ã‚’å®Ÿè¡Œ", type="primary")

# å›ºå®šè¨­å®šï¼ˆUIéè¡¨ç¤ºï¼‰
TOP_K = 20
CTX_N = 5
SHOW_ENQ = False

col_l, col_r = st.columns(2)

if run_btn:
    if not query.strip():
        st.warning("ã‚¯ã‚¨ãƒªã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚")
        st.stop()
    with st.spinner("æ¤œç´¢ä¸­â€¦"):
        en_query = translate_query(query)
        if SHOW_ENQ:
            st.info(f"English query: {en_query}")

        vec = embed_query(en_query)

        filters = {}
        dense_resp, sparse_resp = search_pinecone(vec, en_query, k=TOP_K, filters=filters)

        processed_dense = merge_hits(dense_resp, {"matches": []})
        processed_hybrid = merge_hits(dense_resp, sparse_resp)
        doc_lookup = { str(d["id"]): d for d in processed_hybrid if d.get("id") }

        docs_for_rerank = []
        for d in processed_hybrid:
            rid = str(d.get("id", ""))  # idã¯å¿…ãšæ–‡å­—åˆ—
            if not rid:
                continue
            txt = make_rank_payload(d)
            if not txt:
                continue
            docs_for_rerank.append({"id": rid, "text": txt})

        # â‘  rank_payload ã‚’ä½¿ã†ã‚±ãƒ¼ã‚¹ï¼ˆæ¨å¥¨ï¼‰
        rerank_hybrid = PC.inference.rerank(
            model="pinecone-rerank-v0",
            query=en_query,
            documents=docs_for_rerank,
            rank_fields=["text"],   # â˜… è‘—è€…ãƒ»è‘—ä½œï¼‹è¦ç´„ã ã‘ã‚’èª­ã‚€
            top_n=CTX_N,
            return_documents=True,
            parameters={"truncate": "END"},
        )

        # å·¦: çµæœAï¼ˆDense: ãƒªãƒ©ãƒ³ã‚­ãƒ³ã‚°ãªã—ã€scoreé †ä¸Šä½5ï¼‰
        with col_l:
            st.markdown("### çµæœA")
            if not processed_dense:
                st.warning("è©²å½“ã™ã‚‹æ–‡è„ˆãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚æ¡ä»¶ã‚’èª¿æ•´ã—ã¦ãã ã•ã„ã€‚")
            else:
                html_left = build_sources_html_from_docs(processed_dense[:CTX_N])
                st.markdown(html_left, unsafe_allow_html=True)

        # å³: çµæœBï¼ˆãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰: å†ãƒ©ãƒ³ã‚¯ä¸Šä½5ï¼‰
        with col_r:
            st.markdown("### çµæœB")
            if not rerank_hybrid.data:
                st.warning("è©²å½“ã™ã‚‹æ–‡è„ˆãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚æ¡ä»¶ã‚’èª¿æ•´ã—ã¦ãã ã•ã„ã€‚")
            else:
                html_right = build_sources_html_from_rerank(rerank_hybrid.data, doc_lookup)
                st.markdown(html_right, unsafe_allow_html=True)
else:
    st.info("ä¸Šã®å…¥åŠ›æ¬„ã«ã‚¯ã‚¨ãƒªã‚’å…¥åŠ›ã—ã¦ã€Œæ¤œç´¢ã‚’å®Ÿè¡Œã€ã‚’æŠ¼ã—ã¦ãã ã•ã„ã€‚")

